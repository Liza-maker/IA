from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename
import os
import fitz  # PyMuPDF
import requests
from flask_cors import CORS

from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import ollama
from sentence_transformers import SentenceTransformer
import faiss
import time 

print("D√©but du programme...")
app = Flask(__name__)
#Autoriser la redirection vers les liens adresses suivantes :
CORS(app, resources={r"/*": {"origins": ["http://127.0.0.1:5000", "http://localhost:5001"]}})
UPLOAD_FOLDER = "uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

OLLAMA_URL = "http://localhost:11434/api/generate"

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join(page.get_text() for page in doc)

def chunk_text(text, max_words=500):
    words = text.split()
    return [' '.join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

def generate_summary(prompt):
    response = requests.post(OLLAMA_URL, json={
        "model": "llama3.1",
        "prompt": prompt,
        "temperature":0.2,
        "stream": False
    })
    result = response.json()
    return result.get("response", "").strip()

def readPdf(list_file):
    """
    Fonction qui transforme des pdf en texte :

    Args:
        file (list): liste des fichiers

    Returns:
        text : contenu du fichier
    """
    list_texts = []
    for file in list_file:
        reader = PdfReader(file)
        nb_pages = len(reader.pages)
        for i in range(nb_pages):
            page = reader.pages[i]
            text = page.extract_text()
            list_texts.append(text)
    return list_texts
#document = readPdf(["pdf-exemple.pdf","sample.pdf","rugby.pdf","mes-fiches-animaux-de-la-ferme.pdf","vaches.pdf"])

def chuncking_doc(file):
    """Fonction qui cr√©√© des chunks sur le texte

    Args:
        doc (text): les textes des pdf

    Returns:
        chunks : textes pr√©-trait√©s
    """
    #Fait une d√©coupe plus intelligente que ma fonction gr√¢ce √† chunk_overlap qui va essayer de trouver une phrase avant la d√©coupe.
    docs = [Document(page_content=page_text) for page_text in file]
    splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap=75,separators=["\n\n","\n",".","!","?"])
    splits_docs = splitter.split_documents(docs)
    chunks = [doc.page_content for doc in splits_docs]
    #On enl√®ve tous les sauts de ligne
    def clean_chunk(text):
        return ' '.join(text.replace('\n', ' ').split())
    chunks = [clean_chunk(chunk) for chunk in chunks]
    return chunks


def embedding_texts(chunks,user_text):
    """Fonction qui fait des embeddings sur les chunks

    Args:
        chunks (list): liste des chunks
        user_text (str): question de l'utilisateur

    Returns:
        output_text : Top k (k=4) des similarit√©s entre chunks et question
    """
    print("Chunks : ",chunks)
    if len(chunks) > 0:
        
        #On embedding (transf√©rer les phrases par des vecteurs) (documents puis question utilisateur)
        modelEmbedding = SentenceTransformer("all-MiniLM-L6-v2")
        embedding = modelEmbedding.encode(chunks, show_progress_bar=True, convert_to_numpy=True)

        embedding_user = modelEmbedding.encode([user_text],show_progress_bar=True, convert_to_numpy=True).astype("float32")    
        dimension = embedding.shape[1]
        index = faiss.IndexFlatIP(dimension)  # 32 = nombre de voisins (standard)
        output_text = []
        #on ajoute les embeddings : 
        index.add(embedding)
        distances, indices = index.search(embedding_user, k=4)
        for i, idx in enumerate(indices[0]):
            output_text.append(chunks[idx])
        return output_text
    return False

def predict_with_ollama(context, question, model_name="llama3.1"):
    """Fonction qui g√©n√®re le r√©sultat du mod√®le

    Args:
        context (list)  : chunks les + similaires √† la question
        question (str)  : question de l'utilisateur
        model_name (str): mod√®le utilis√©

    Returns:
        result["response"] : R√©ponse du mod√®le √† la question de l'utilisateur
    """
    print(context)
    prompt = f"""Voici des extraits de documents :\n{context}\n\nQuestion : {question}\n :
    R√©pond en utilisant principalement les extraits de document et en reformulant.
    N'inclus ni ton avis ni ton analyse.
    Si la question n'a pas de rapport r√©pond : 'D√©sol√© les documents ne r√©pondent pas √† votre question'.\n
    R√©ponse : """
    result = ollama.generate(model=model_name, prompt= prompt)
    return result["response"]

#=================================================================
#               D√©but des app.route
#=================================================================
@app.route('/questionUtilisateur', methods=['POST'])
def questionUtilisateur():
    file = request.files['file']
    text_user = request.form['question']
    if file and text_user:
            docs = readPdf([file])
            chunks = chuncking_doc(docs)
            embeddings_text = embedding_texts(chunks,text_user)
            if not embeddings_text:
                return jsonify({"summary": "D√©sol√©, le fichier n'est pas adapt√© √† l'extraction de texte."})
            l = predict_with_ollama(embeddings_text,text_user)
            return jsonify({"summary": l})



@app.route('/upload', methods=['POST'])
def upload():
    start_time = time.time()

    file = request.files['file']
    platform = request.form.get('platform', 'g√©n√©rique')
    filter_style = request.form.get('style_filter', '')
    extra_prompt = request.form.get('custom_prompt', '').strip()
    print("Informations re√ßus !")
    base_prompts = {
        "Instagram": "Cr√©e un r√©sum√© percutant pour une story Instagram, en fran√ßais. Utilise des √©mojis üì∏‚ú®, des phrases courtes et visuelles. Le texte doit √™tre pr√™t √† √™tre publi√©.",
        "Facebook": "Cr√©e un r√©sum√© informatif et engageant pour une publication Facebook, en fran√ßais.",
        "Linkedin": "Fais un r√©sum√© tr√®s court et professionnel (moins de 280 caract√®res) pour Linkedin.",
        "Site web": "Cr√©e un r√©sum√© clair, professionnel et structur√© pour un site web.",
        "Presse": "R√©dige un court communiqu√© de presse √† partir de ce document.",
        "g√©n√©rique": "Fais un r√©sum√© court et clair de ce document PDF."
    }

    filter_prompts = {
        "attrayant": "Rends ce r√©sum√© tr√®s attrayant et captivant.",
        "dr√¥le": "Ajoute une touche d'humour et rends ce r√©sum√© dr√¥le.",
        "cr√©atif": "Sois cr√©atif et original dans le r√©sum√©.",
        "professionnel": "Rends ce r√©sum√© tr√®s professionnel et s√©rieux."
    }

    if file and file.filename.endswith('.pdf'):
        filename = secure_filename(file.filename)
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        file.save(filepath)

        text = extract_text_from_pdf(filepath)
        chunks = chunk_text(text)
        print("Chunks effectu√©s pour r√©sumer !")
        all_summaries = []
        for chunk in chunks:
            prompt_parts = [base_prompts.get(platform, base_prompts['g√©n√©rique'])]
            if filter_style in filter_prompts:
                prompt_parts.append(filter_prompts[filter_style])
            if extra_prompt:
                prompt_parts.append(extra_prompt)
            prompt_parts.append(f"\nTexte :\n{chunk}\n\nR√©sum√© :")

            full_prompt = "\n".join(prompt_parts)
            summary = generate_summary(full_prompt)
            all_summaries.append(summary)

        duration = round(time.time() - start_time, 2)
        return jsonify({"summary": "\n\n".join(all_summaries), "duration": duration})
    else:
        return "Format non pris en charge", 400





if __name__ == '__main__':
    app.run(debug=True)