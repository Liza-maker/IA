{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c95414aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def readPdf(list_file):\n",
    "    \"\"\"\n",
    "    Fonction qui transforme des pdf en texte :\n",
    "\n",
    "    Args:\n",
    "        file (list): liste des fichiers\n",
    "\n",
    "    Returns:\n",
    "        text : contenu du fichier\n",
    "    \"\"\"\n",
    "    list_texts = []\n",
    "    for file in list_file:\n",
    "        reader = PdfReader(file)\n",
    "        nb_pages = len(reader.pages)\n",
    "        for i in range(nb_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            list_texts.append(text)\n",
    "    return list_texts\n",
    "\n",
    "\n",
    "\n",
    "document = readPdf([\"pdf-exemple.pdf\",\"sample.pdf\",\"rugby.pdf\",\"mes-fiches-animaux-de-la-ferme.pdf\",\"vaches.pdf\"])\n",
    "#document = readPdf([])\n",
    "\n",
    "docs = [Document(page_content=page_text) for page_text  in document]\n",
    "#Fait une découpe plus intelligente que ma fonction grâce à chunk_overlap qui va essayer de trouver une phrase avant la découpe.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap=75,separators=[\"\\n\\n\",\"\\n\",\".\",\"!\",\"?\"])\n",
    "splits_docs = splitter.split_documents(docs)\n",
    "chunks = [doc.page_content for doc in splits_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0910cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 :\n",
      "Fichier PDF d'exemple \n",
      "Le Portable Document Format (qui se traduit de l'anglais en « format de document \n",
      "portable »), généralement abrégé PDF, est un format de fichier informatique créé par \n",
      "Adobe  Systems.  C'est  un  format  ouvert  dont  les  spécifications  sont  publiques  et \n",
      "utilisables librement (certains éléments sont à disposition sur le site Adobe). Il est\n",
      "--------------------------------------------------\n",
      "Chunk 1 :\n",
      "dérivé du format PostScript et contient des données au format XML.\n",
      "Le format PDF est un format de fichier qui préserve les polices, les images, les objets \n",
      "graphiques et la mise en forme de tout document source, quelles que soient l'application \n",
      "et la plate-forme utilisées pour le créer. Les fichiers PDF peuvent être créés avec des\n",
      "--------------------------------------------------\n",
      "Chunk 2 :\n",
      "options personnalisées, tant aux niveaux de la compression des images et des textes, \n",
      "de la qualité d'impression du fichier, ainsi que du verrouillage (interdiction d'impression, \n",
      "de modification...).\n",
      "Le format PDF n'est pas un format statique mais un format interactif. Il est en effet \n",
      "possible  (grâce  à  Acrobat  Pro)  d'incorporer  des  champs  de  textes,  des  menus\n",
      "--------------------------------------------------\n",
      "Chunk 3 :\n",
      "déroulants, des choix, des calculs... sur un document universel PDF : on parle alors de \n",
      "formulaire PDF.\n",
      "Le PDF est consultable sur de très nombreux appareils communicants (ordinateurs, \n",
      "assistants personnels numériques / PDA, nouveaux téléphones hybrides...). En effet, le \n",
      "lecteur  gratuit,  nommé  «  Adobe  Reader  »  est disponible  sur  de  très  nombreuses\n",
      "--------------------------------------------------\n",
      "Chunk 4 :\n",
      "plateformes et systèmes d'exploitations : Mac OS, Windows, Linux, Palm OS, Pocket PC, \n",
      "Symbian OS, Sun Solaris Sparc, IBM AIX, HP-UX, OS/2 / Warp... De nombreux autes \n",
      "lecteurs dont certains sont des logiciels libres, existent également. La génération de \n",
      "documents dans ce format est possible à l'aide de logiciels spécialisés, d'imprimantes\n",
      "--------------------------------------------------\n",
      "Chunk 5 :\n",
      "virtuelles  mais  elle  est  également  possible  automatiquement  dans  certaines  suites \n",
      "bureautiques.  C'est  ainsi  que  ce  format  universel  est  considéré  comme  le  format \n",
      "mondial d'échange (et d'archivage) de documents électroniques.\n",
      "Il existe des variantes du format PDF de base, dit PDF 1.3 ou 1.4, des versions «\n",
      "--------------------------------------------------\n",
      "Chunk 6 :\n",
      "rastérisées » (la rastérisation consiste à transformer les textes modifiables en images \n",
      "matricielles figées accompagnées des images d'illustration) dites PDF-IT et PDF-X. Cette \n",
      "version est principallement utilisée dans l'industrie graphique lors de la séparation des \n",
      "couleurs en quadrichromie, au niveau du RIP.\n",
      "--------------------------------------------------\n",
      "Chunk 7 :\n",
      "couleurs en quadrichromie, au niveau du RIP.\n",
      "Une nouvelle forme de fichiers PDF voit actuellement le jour. Dans ce dernier, le fichier \n",
      "image du texte est transformé en vecteurs au lieu d'un fichier bitmap, ce qui permet \n",
      "l'allégement du fichier ainsi que la capacité d'être agrandi à l'envie.\n",
      "--------------------------------------------------\n",
      "Chunk 8 :\n",
      "Sample PDFThis is a simple PDF ﬁle. Fun fun fun.\n",
      "Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. \n",
      "Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget \n",
      "pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. \n",
      "Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus.\n",
      "--------------------------------------------------\n",
      "Chunk 9 :\n",
      "Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla \n",
      "erat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. \n",
      "Vivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique \n",
      "accumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget,\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#On analyse nos chunks : \n",
    "for i, chunk in enumerate(chunks[:10]):\n",
    "    print(f\"Chunk {i} :\\n{chunk}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bdc0172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cfdfec0491446583692460869418b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390e0f6e515345e1b1c64d7fd3cbebf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#On embedding (transférer les phrases par des vecteurs)\n",
    "modelEmbedding = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedding = modelEmbedding.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "user_text = input(\"Entrez votre phrase : \")\n",
    "embedding_user = modelEmbedding.encode([user_text],show_progress_bar=True, convert_to_numpy=True).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f8cc70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question de l'utilisateur : Qui a gagné a coupe du monde de rugby en 1982 ?\n",
      "\n",
      "1. Score = 0.54 | Les règle du rugby\n",
      "La durée:la durée d'un match de rugby est de 2x 40minutes pour les  \n",
      "séniors. \n",
      "les   règles:  il doit y avoir 15 joueurs obligatoirement dans une équipe de \n",
      "rugby.\n",
      "Un match commence par un coup d’envoi.\n",
      "Après le coup d’envoi, tout joueur qui est en jeu peut se saisir du ballon et \n",
      "courir en le portant.\n",
      "Un joueur peut lancer le ballon ou le botter.\n",
      "2. Score = 0.43 | Noordhuizen, 2003) \n",
      " \n",
      "Score 1 \n",
      "Flanc gauche très creux. Le pli de \n",
      "peau sous la pointe de la hanche \n",
      "tombe verticalement. \n",
      "La vache n’a pas mangé ou a peu \n",
      "mangé. \n",
      "0\n",
      "0,5\n",
      "1\n",
      "1,5\n",
      "2\n",
      "2,5\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "5\n",
      "3. Score = 0.42 | • Hors-jeu  :quand on et en avant du ballon.\n",
      "• Mêlée  :huit joueurs de chaque équipe, liés entre eux sur trois lignes \n",
      "pour chaque équipe. \n",
      "• Pénalité  :La pénalité est un geste technique.\n",
      "• Touch  e:  il faut lancer le ballons a la quand on et en touche.\n",
      "VOICI QUELQUE PHOTO DE BALLONS DE RUGBY:\n",
      "4. Score = 0.42 | Un joueur peut faire un touché à terre dans un en-but.\n",
      "Un porteur du ballon peut effectuer un raffut sur un adversaire.\n",
      "Toute action d’un joueur doit être faite dans le respect des Règles du Jeu.\n",
      "Un joueur et obliger de passer la balle en arrière\n",
      " \n",
      "Un terrain de rugby:\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "dimension = embedding.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # 32 = nombre de voisins (standard)\n",
    "output_text = []\n",
    "\n",
    "#on ajoute les embeddings : \n",
    "index.add(embedding)\n",
    "print(f\"Question de l'utilisateur : {user_text}\\n\")\n",
    "distances, indices = index.search(embedding_user, k=4)\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    score = math.ceil(distances[0][i]*100)/100\n",
    "    print(f\"{i+1}. Score = {distances[0][i]:.2f} | {chunks[idx]}\")\n",
    "    output_text.append(chunks[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d18a8877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#On utilise la recherche sémantique pour retrouver les vecteurs les plus proches (donc les plus probables)\\nresult = util.semantic_search(embedding_user,embedding)[0]\\nprint(result)\\noutput_text = []\\n\\n#On inclut une réponse par défaut si la similarité est pas assez haute pour répondre à la question posée.\\n\\nfor item in result:\\n    score = math.ceil(item[\"score\"]*100)/100\\n    print(score, \"|\", chunks[item[\"corpus_id\"]])\\n\\n    #On fait un top 10 des réponses les plus probables et qui sont le plus probable de sortir (arrondi >0.6)\\n    if len(output_text) < 10 and score >= 0.6:\\n        output_text.append(chunks[item[\"corpus_id\"]])\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#On utilise la recherche sémantique pour retrouver les vecteurs les plus proches (donc les plus probables)\n",
    "result = util.semantic_search(embedding_user,embedding)[0]\n",
    "print(result)\n",
    "output_text = []\n",
    "\n",
    "#On inclut une réponse par défaut si la similarité est pas assez haute pour répondre à la question posée.\n",
    "\n",
    "for item in result:\n",
    "    score = math.ceil(item[\"score\"]*100)/100\n",
    "    print(score, \"|\", chunks[item[\"corpus_id\"]])\n",
    "    \n",
    "    #On fait un top 10 des réponses les plus probables et qui sont le plus probable de sortir (arrondi >0.6)\n",
    "    if len(output_text) < 10 and score >= 0.6:\n",
    "        output_text.append(chunks[item[\"corpus_id\"]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f86457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1747d888fb343578c0dd22300b6690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC7\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC7\\.cache\\huggingface\\hub\\models--google--flan-t5-xl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fcadd4801a4cb0a21473ed2a3bf48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f744ef72813e41aea9d0c102055491a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705ec94bbd5543d3a7c5e9ade0a1e25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71a5d06af6544da9dca75b47f028db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d90f1d08ad7423490090e1baeee00af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684fcc8e934f4a28a631a7252a5119c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6b878e150b4534aec26f23d992a9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed12a9eaa2a40178eebd2b5ebbab25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoModelForSeq2SeqLM\n",
    "import time\n",
    "from datetime import datetime\n",
    "from csv import writer\n",
    "\n",
    "t0 = time.time()\n",
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#model_name = \"google/flan-t5-large\" Très très bon déjà mais limite 512 tokens\n",
    "model_name = \"google/flan-t5-xl\"\n",
    "#\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "DEFAULT_RESPONSE = \"Désolé, les fichiers ne permettent pas de répondre à vos questions.\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Voici des extraits de documents jugés pertinents pour répondre à la question ci-dessous. Reformule ta réponse de manière claire.\n",
    "Extraits : \n",
    "\"\"\" + str(output_text) +  \"\"\"\\n\\n\n",
    "Consignes : \\n\n",
    "- Ne génère ta réponse qu'en te basant strictement sur le contenu des documents ci-dessus.\n",
    "- Si les documents ne permettent pas de répondre clairement à la question, dis simplement : \n",
    "\"Je ne peux pas répondre à cette question avec les documents fournis.\"\n",
    "Question :\n",
    "\"\"\"+ user_text  +\"\"\"\n",
    "Réponse:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if len(output_text) > 0 and len(document) > 0:\n",
    "    #On remplace les mots par des vecteurs : \n",
    "    inputs = tokenizer(prompt_template,return_tensors=\"pt\")\n",
    "    print(\"Longueur token du prompt : \",inputs['input_ids'].shape[1])\n",
    "    eos_token_id = tokenizer.eos_token_id  # Utilise le token EOS pour arrêter la génération\n",
    "\n",
    "    # Permet de faire répondre le modèle sur les vecteurs que l'on vient de créer\n",
    "    outputs = model.generate(**inputs, temperature=0.1, max_new_tokens=200, eos_token_id=eos_token_id)\n",
    "\n",
    "    #On décode la réponse renvoyée par le modèle : \n",
    "    response = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    print(response)\n",
    "    \n",
    "    #Pour les stats :\n",
    "    result_response = \"Réponse correcte\"\n",
    "elif len(document) == 0:\n",
    "    prompt_template = \"\"\"\n",
    "    Répond à la question :\"\"\" + user_text\n",
    "    \n",
    "    #On remplace les mots par des vecteurs : \n",
    "    inputs = tokenizer(prompt_template,return_tensors=\"pt\")\n",
    "    print(\"Longueur token du prompt : \",tokenizer['input_ids'].shape[1])\n",
    "    eos_token_id = tokenizer.eos_token_id  # Utilise le token EOS pour arrêter la génération\n",
    "\n",
    "    # Permet de faire répondre le modèle sur les vecteurs que l'on vient de créer\n",
    "    outputs = model.generate(**inputs, temperature=0.1, max_new_tokens=500, eos_token_id=eos_token_id)\n",
    "\n",
    "    #On décode la réponse renvoyée par le modèle : \n",
    "    response = tokenizer.decode(outputs[0],skip_special_tokens=True) + \"\\n⚠️ CETTE REPONSE A ETE FOURNI SANS DOCUMENT ANNEXE, ELLE EST DONC GENERALE ⚠️\"\n",
    "    print(response)\n",
    "    \n",
    "    #Pour les stats :\n",
    "    result_response = \"Réponse hors-sujet sans fichier\"\n",
    "else:\n",
    "    #Si il n'y a aucune réponse assez similaire (seuil 0.6)\n",
    "    response = DEFAULT_RESPONSE\n",
    "    result_response = \"Hors-sujet\"\n",
    "t1 = time.time()\n",
    "total_time = round(t1-t0,5)\n",
    "\n",
    "#Inscription de la date : \n",
    "date_actuelle = datetime.now().strftime(\"%d/%m, %H:%M:%S\")\n",
    "# Inscription dans le fichier txt : \n",
    "with open(\"data_saved_v2.txt\", \"a\", newline=\"\", encoding=\"utf-8\") as fichier:\n",
    "    fichier.write(\"-------------------------------------------------------------------\\n\")\n",
    "    fichier.write(\"Test du : \" + date_actuelle + \" avec modèle : \" + model_name + \"\\n\")\n",
    "    fichier.write(\"-------------------------------------------------------------------\\n\")\n",
    "    fichier.write(\"Temps d'éxecution : \" + str(total_time) + \"\\n\")\n",
    "    fichier.write(\"Question posée : \" + user_text + \"\\n\")\n",
    "    fichier.write(\"Réponse apportée : \" + response + \"\\n\")\n",
    "    fichier.write(\"===================================================================\\n\\n\")\n",
    "fichier.close()\n",
    "\n",
    "\n",
    "\n",
    "#Pareil mais en csv : \n",
    "dictio_csv = [model_name,date_actuelle,total_time,result_response]\n",
    "with open(\"data_saved.csv\",\"a\",encoding=\"utf-8\",newline=\"\") as fichier:\n",
    "    w = writer(fichier)\n",
    "    w.writerow(dictio_csv)\n",
    "    fichier.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
